{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seedevk8s/006895/blob/master/110_step_by_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "R_HUeHmj_-5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers"
      ],
      "metadata": {
        "id": "STBIogA1_ObE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soDbvb1v-8oG"
      },
      "outputs": [],
      "source": [
        "from diffusers import DDPMPipeline\n",
        "\n",
        "ddpm = DDPMPipeline.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")\n",
        "image = ddpm(num_inference_steps=25).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import DDPMScheduler, UNet2DModel\n",
        "\n",
        "scheduler = DDPMScheduler.from_pretrained(\"google/ddpm-cat-256\")\n",
        "model = UNet2DModel.from_pretrained(\"google/ddpm-cat-256\", use_safetensors=True).to(\"cuda\")"
      ],
      "metadata": {
        "id": "SocLP2Z9AR2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.set_timesteps(50)"
      ],
      "metadata": {
        "id": "C2aZo1-iA8Gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.timesteps"
      ],
      "metadata": {
        "id": "E0eCi05wBCzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "sample_size = model.config.sample_size\n",
        "noise = torch.randn((1, 3, sample_size, sample_size)).to(\"cuda\")"
      ],
      "metadata": {
        "id": "UXc8S5FMBId1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = noise\n",
        "\n",
        "for t in scheduler.timesteps:\n",
        "    with torch.no_grad():\n",
        "        noisy_residual = model(input, t).sample\n",
        "    previous_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n",
        "    input = previous_noisy_sample"
      ],
      "metadata": {
        "id": "o8sARIVlBQmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "image = (input / 2 + 0.5).clamp(0, 1).squeeze()\n",
        "image = (image.permute(1, 2, 0) * 255).round().to(torch.uint8).cpu().numpy()\n",
        "image = Image.fromarray(image)\n",
        "image"
      ],
      "metadata": {
        "id": "qaWWsQf_BWA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "FaBiWMmrCB3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_safetensors=True)\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"text_encoder\", use_safetensors=True\n",
        ")\n",
        "unet = UNet2DConditionModel.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_safetensors=True\n",
        ")"
      ],
      "metadata": {
        "id": "EYqUSXowB9tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UniPCMultistepScheduler\n",
        "\n",
        "scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
      ],
      "metadata": {
        "id": "tfKqtAqBCQrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_device = \"cuda\"\n",
        "vae.to(torch_device)\n",
        "text_encoder.to(torch_device)\n",
        "unet.to(torch_device)"
      ],
      "metadata": {
        "id": "fmXoXLtICW2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
        "height = 512  # default height of Stable Diffusion\n",
        "width = 512  # default width of Stable Diffusion\n",
        "num_inference_steps = 25  # Number of denoising steps\n",
        "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(0)  # Seed generator to create the inital latent noise\n",
        "batch_size = len(prompt)"
      ],
      "metadata": {
        "id": "mR94eEzgCcQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = tokenizer(\n",
        "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]"
      ],
      "metadata": {
        "id": "Ot28DGHCChcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]"
      ],
      "metadata": {
        "id": "v2G7Fd3jCmj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])"
      ],
      "metadata": {
        "id": "cSuLbHrOCsXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latents = torch.randn(\n",
        "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)"
      ],
      "metadata": {
        "id": "k5xFoL-OCw6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latents = latents * scheduler.init_noise_sigma"
      ],
      "metadata": {
        "id": "-9axnpGhC0c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "for t in tqdm(scheduler.timesteps):\n",
        "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "    latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
        "\n",
        "    # predict the noise residual\n",
        "    with torch.no_grad():\n",
        "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "    # perform guidance\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    # compute the previous noisy sample x_t -> x_t-1\n",
        "    latents = scheduler.step(noise_pred, t, latents).prev_sample"
      ],
      "metadata": {
        "id": "Sx6fv6DpDBKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "with torch.no_grad():\n",
        "    image = vae.decode(latents).sample"
      ],
      "metadata": {
        "id": "C8ooL0ZWDJ4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
        "image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "image = Image.fromarray(image)\n",
        "image"
      ],
      "metadata": {
        "id": "zjOuIpGEDMLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForText2Image\n",
        "import torch\n",
        "\n",
        "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "prompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"\n",
        "\n",
        "image = pipeline(prompt, num_inference_steps=25).images[0]"
      ],
      "metadata": {
        "id": "io4dAfhcFzat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "dr5uV54WGFb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "id": "w4tcNYfEGY_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForImage2Image\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        ").to(\"cuda\")\n",
        "prompt = \"a portrait of a dog wearing a pearl earring\"\n",
        "\n",
        "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/1665_Girl_with_a_Pearl_Earring.jpg/800px-1665_Girl_with_a_Pearl_Earring.jpg\"\n",
        "\n",
        "response = requests.get(url)\n",
        "image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "image.thumbnail((768, 768))\n",
        "\n",
        "image = pipeline(prompt, image, num_inference_steps=200, strength=0.75, guidance_scale=10.5).images[0]"
      ],
      "metadata": {
        "id": "OgbHJAO3GPzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "5VCGmbAaG0Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForInpainting\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "pipeline = AutoPipelineForInpainting.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, use_safetensors=True\n",
        ").to(\"cuda\")\n",
        "\n",
        "img_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\n",
        "mask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n",
        "\n",
        "init_image = load_image(img_url).convert(\"RGB\")\n",
        "mask_image = load_image(mask_url).convert(\"RGB\")\n",
        "\n",
        "prompt = \"A majestic tiger sitting on a bench\"\n",
        "image = pipeline(prompt, image=init_image, mask_image=mask_image, num_inference_steps=50, strength=0.80).images[0]"
      ],
      "metadata": {
        "id": "S10ylSjLH3zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "vkdwZshbIZrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n",
        "\n",
        "pipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n",
        ")\n",
        "print(type(pipeline_text2img))\n"
      ],
      "metadata": {
        "id": "2e_7NPR4Iakt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)\n",
        "print(type(pipeline_img2img))\n"
      ],
      "metadata": {
        "id": "_h0TyNp-I1Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n",
        "\n",
        "pipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    requires_safety_checker=False,\n",
        ").to(\"cuda\")\n",
        "\n",
        "pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)\n",
        "print(pipeline_img2img.config.requires_safety_checker)"
      ],
      "metadata": {
        "id": "cpDdcpSaI8tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img, requires_safety_checker=True, strength=0.3)\n",
        "print(pipeline_img2img.config.requires_safety_checker)"
      ],
      "metadata": {
        "id": "iFQLKKIoJHaS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}